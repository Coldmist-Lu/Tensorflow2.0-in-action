{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Self-defined Model\n",
    "\n",
    "* 本代码对应笔记（四）：自定义模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1：求导机制的简单例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=11, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x) # watch:确保某个Tensor被tape追踪\n",
    "    y = x * x\n",
    "dy_dx = g.gradient(y, x) # gradient:根据tape计算某个或某些Tensor的梯度\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2：使用 GradientTape 自定义训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training loss (for one batch) at step 0: 24.458969116210938\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 19.38613510131836\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 17.059972763061523\n",
      "Seen so far: 64 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # 打开GradientTape以记录正向传递期间运行的操作，这将启用自动区分。\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # 运行该模型的前向传播。 模型应用于其输入的操作将记录在GradientTape上。\n",
    "            logits = model(x_batch_train, training=True)  # 这个minibatch的预测值\n",
    "\n",
    "            # 计算这个minibatch的损失值\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # 使用GradientTape自动获取可训练变量相对于损失的梯度。\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # 通过更新变量的值来最大程度地减少损失，从而执行梯度下降的一步。\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 每200 batches打印一次.\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3：自定义模型进阶（加入评估函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        #定义前向传播\n",
    "        # 使用在 (in `__init__`)定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000, 32))\n",
    "y_train = np.random.random((1000, 10))\n",
    "x_val = np.random.random((200, 32))\n",
    "y_val = np.random.random((200, 10))\n",
    "x_test = np.random.random((200, 32))\n",
    "y_test = np.random.random((200, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# 损失函数\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 准备metrics函数\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# 准备训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# 准备测试数据集\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 循环训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training acc over epoch: 0.10100000351667404\n",
      "Validation acc: 0.0949999988079071\n",
      "Start of epoch 1\n",
      "Training acc over epoch: 0.09799999743700027\n",
      "Validation acc: 0.0949999988079071\n",
      "Start of epoch 2\n",
      "Training acc over epoch: 0.09600000083446503\n",
      "Validation acc: 0.09000000357627869\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                \n",
    "        # 对每一个batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) # 通过优化器更新权重\n",
    "\n",
    "        # 更新训练集的metrics\n",
    "        train_acc_metric(y_batch_train, logits) # 更新训练集的logits     \n",
    "            \n",
    "    # 在每个epoch结束时显示metrics。\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    \n",
    "    # 在每个epoch结束时重置训练指标\n",
    "    train_acc_metric.reset_states() # 非常重要，不能漏掉！\n",
    "\n",
    "    # 在每个epoch结束时运行一个验证集。\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # 更新验证集metrics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    print('Validation acc: %s' % (float(val_acc),))\n",
    "    val_acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
