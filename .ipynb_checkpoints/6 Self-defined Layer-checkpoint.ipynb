{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-defined Layer\n",
    "\n",
    "* 本代码对应笔记（七）：自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1：自定义层初探"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(issubclass(tf.keras.Model,tf.Module))\n",
    "print(issubclass(tf.keras.layers.Layer,tf.Module))\n",
    "print(issubclass(tf.keras.Model,tf.keras.layers.Layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__() #\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果与参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00253096  0.02058197 -0.14340366  0.06595445]\n",
      " [-0.00253096  0.02058197 -0.14340366  0.06595445]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[-0.03913387,  0.02152313, -0.06586333,  0.0331893 ],\n",
       "        [ 0.03660291, -0.00094116, -0.07754033,  0.03276515]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       "array([[-0.03913387,  0.02152313, -0.06586333,  0.0331893 ],\n",
       "       [ 0.03660291, -0.00094116, -0.07754033,  0.03276515]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2：自定义层的构造实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集导入和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=1, input_dim=4):\n",
    "        super(Linear, self).__init__() # 先调用基类初始化函数\n",
    "        w_init = tf.random_normal_initializer() # 进行变量随机初始化\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                                  dtype='float32'), \n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer() # 进行变量全零初始化\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer linear_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(data) # (150,4)\n",
    "linear_layer = Linear(units = 1, input_dim=4) # ()\n",
    "y = linear_layer(x)\n",
    "print(y.shape) # (150,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.35666478]\n",
      " [-0.31824857]\n",
      " [-0.32514453]\n",
      " [-0.31008494]\n",
      " [-0.358857  ]\n",
      " [-0.36314693]\n",
      " [-0.31983596]\n",
      " [-0.34411675]\n",
      " [-0.2925234 ]\n",
      " [-0.33295506]\n",
      " [-0.3781485 ]\n",
      " [-0.3337609 ]\n",
      " [-0.3254301 ]\n",
      " [-0.31335384]\n",
      " [-0.41971487]\n",
      " [-0.41050836]\n",
      " [-0.37319306]\n",
      " [-0.34556112]\n",
      " [-0.37990263]\n",
      " [-0.36139274]\n",
      " [-0.3547823 ]\n",
      " [-0.3441747 ]\n",
      " [-0.35321444]\n",
      " [-0.30359048]\n",
      " [-0.32622632]\n",
      " [-0.31714767]\n",
      " [-0.31939793]\n",
      " [-0.3580754 ]\n",
      " [-0.35447258]\n",
      " [-0.31760994]\n",
      " [-0.31541774]\n",
      " [-0.3375981 ]\n",
      " [-0.40586534]\n",
      " [-0.41515407]\n",
      " [-0.3218514 ]\n",
      " [-0.33942252]\n",
      " [-0.37486494]\n",
      " [-0.3660385 ]\n",
      " [-0.3011493 ]\n",
      " [-0.34803888]\n",
      " [-0.3441505 ]\n",
      " [-0.25116715]\n",
      " [-0.31337804]\n",
      " [-0.30330497]\n",
      " [-0.340243  ]\n",
      " [-0.30322278]\n",
      " [-0.36998487]\n",
      " [-0.31871083]\n",
      " [-0.37422633]\n",
      " [-0.34051386]\n",
      " [-0.19671848]\n",
      " [-0.16710493]\n",
      " [-0.1705552 ]\n",
      " [-0.111541  ]\n",
      " [-0.14405799]\n",
      " [-0.13739958]\n",
      " [-0.15317042]\n",
      " [-0.14501406]\n",
      " [-0.1763019 ]\n",
      " [-0.1156399 ]\n",
      " [-0.11945564]\n",
      " [-0.14279994]\n",
      " [-0.15834837]\n",
      " [-0.14307588]\n",
      " [-0.16219556]\n",
      " [-0.18637216]\n",
      " [-0.12349889]\n",
      " [-0.17856444]\n",
      " [-0.09811679]\n",
      " [-0.15241073]\n",
      " [-0.10664856]\n",
      " [-0.16564587]\n",
      " [-0.11033598]\n",
      " [-0.15916882]\n",
      " [-0.17599213]\n",
      " [-0.17633563]\n",
      " [-0.16190508]\n",
      " [-0.1318777 ]\n",
      " [-0.13307312]\n",
      " [-0.18359701]\n",
      " [-0.1448857 ]\n",
      " [-0.1585009 ]\n",
      " [-0.16138016]\n",
      " [-0.09467156]\n",
      " [-0.11565456]\n",
      " [-0.15254141]\n",
      " [-0.16773394]\n",
      " [-0.13287216]\n",
      " [-0.1557523 ]\n",
      " [-0.12376973]\n",
      " [-0.13094167]\n",
      " [-0.15170181]\n",
      " [-0.15275429]\n",
      " [-0.14282183]\n",
      " [-0.13489762]\n",
      " [-0.16826658]\n",
      " [-0.15104854]\n",
      " [-0.1681478 ]\n",
      " [-0.15540369]\n",
      " [-0.14744568]\n",
      " [-0.02058768]\n",
      " [-0.05351627]\n",
      " [-0.08054797]\n",
      " [-0.08390185]\n",
      " [-0.04842284]\n",
      " [-0.08257814]\n",
      " [-0.04326454]\n",
      " [-0.10554275]\n",
      " [-0.07010993]\n",
      " [-0.07171872]\n",
      " [-0.10043964]\n",
      " [-0.07202622]\n",
      " [-0.07882763]\n",
      " [-0.02877322]\n",
      " [-0.00411233]\n",
      " [-0.05818349]\n",
      " [-0.10037211]\n",
      " [-0.12180009]\n",
      " [-0.03230086]\n",
      " [-0.07771482]\n",
      " [-0.06774819]\n",
      " [-0.04570571]\n",
      " [-0.0828636 ]\n",
      " [-0.08925381]\n",
      " [-0.08822556]\n",
      " [-0.12749833]\n",
      " [-0.09395751]\n",
      " [-0.0997526 ]\n",
      " [-0.04839869]\n",
      " [-0.14249994]\n",
      " [-0.09726995]\n",
      " [-0.1593863 ]\n",
      " [-0.03729501]\n",
      " [-0.12365606]\n",
      " [-0.10212898]\n",
      " [-0.07685056]\n",
      " [-0.04785183]\n",
      " [-0.10256431]\n",
      " [-0.09834197]\n",
      " [-0.09137566]\n",
      " [-0.04519728]\n",
      " [-0.07670292]\n",
      " [-0.05351627]\n",
      " [-0.05880296]\n",
      " [-0.04381093]\n",
      " [-0.06023273]\n",
      " [-0.06340986]\n",
      " [-0.08569935]\n",
      " [-0.06005639]\n",
      " [-0.08688524]], shape=(150, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=1, input_dim=4):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units), # 增加权重\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,), # 增加权重\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer linear_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(data)\n",
    "linear_layer = Linear(units = 1, input_dim=4)\n",
    "y = linear_layer(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape): # 放到build方法中去\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(Linear,self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer linear_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(data) # (150,4)\n",
    "linear_layer = Linear(units = 1)\n",
    "y = linear_layer(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看不可训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        super(Linear,self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer linear_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(data)\n",
    "linear_layer = Linear(units = 1)\n",
    "y = linear_layer(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [<tf.Variable 'linear_4/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[ 0.07387361],\n",
      "       [-0.00789351],\n",
      "       [-0.02528643],\n",
      "       [-0.02258838]], dtype=float32)>, <tf.Variable 'linear_4/Variable:0' shape=(1,) dtype=float32, numpy=array([0.02117108], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print('weight:', linear_layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-trainable weight: [<tf.Variable 'linear_4/Variable:0' shape=(1,) dtype=float32, numpy=array([0.02117108], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print('non-trainable weight:', linear_layer.non_trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weight: [<tf.Variable 'linear_4/Variable:0' shape=(4, 1) dtype=float32, numpy=\n",
      "array([[ 0.07387361],\n",
      "       [-0.00789351],\n",
      "       [-0.02528643],\n",
      "       [-0.02258838]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print('trainable weight:', linear_layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3：自定义层的完整训练实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer): # 注意点四：不能使用与内置层相同的名称\n",
    "    def __init__(self, units=32, **kwargs): # 添加**kwargs\n",
    "        self.units = units\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "\n",
    "    # build方法一般定义Layer需要被训练的参数。    \n",
    "    def build(self, input_shape): \n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name='w') # 必须对可训练层设置名字\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name='b')\n",
    "        super(MyDense,self).build(input_shape) # 相当于设置self.built = True\n",
    "\n",
    "    # call方法一般定义正向传播运算逻辑，__call__方法调用了它。    \n",
    "    def call(self, inputs): \n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    # 如果要让自定义的Layer通过Functional API 组合成模型时可以序列化，需要自定义get_config方法。\n",
    "    def get_config(self):  # 增设get_config方法\n",
    "        config = super(MyDense, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(4,))  \n",
    "x = MyDense(units=16)(inputs) \n",
    "x = tf.nn.tanh(x) \n",
    "x = MyDense(units=3)(x) # 0,1,2\n",
    "predictions = tf.nn.softmax(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打乱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data,labels.reshape(150,1)),axis=-1)\n",
    "np.random.shuffle(data)\n",
    "labels = data[:,-1]\n",
    "data = data[:,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编译并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/100\n",
      "150/150 [==============================] - 3s 20ms/sample - loss: 1.0946 - sparse_categorical_accuracy: 0.3067\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 312us/sample - loss: 1.0918 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 279us/sample - loss: 1.0892 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 346us/sample - loss: 1.0867 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 372us/sample - loss: 1.0839 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 379us/sample - loss: 1.0813 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 319us/sample - loss: 1.0781 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 339us/sample - loss: 1.0747 - sparse_categorical_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 1.0712 - sparse_categorical_accuracy: 0.3400\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 332us/sample - loss: 1.0668 - sparse_categorical_accuracy: 0.4133\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 339us/sample - loss: 1.0624 - sparse_categorical_accuracy: 0.5800\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 1.0572 - sparse_categorical_accuracy: 0.6600\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 1.0516 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 266us/sample - loss: 1.0451 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 552us/sample - loss: 1.0380 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 359us/sample - loss: 1.0303 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 352us/sample - loss: 1.0221 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 293us/sample - loss: 1.0134 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 406us/sample - loss: 1.0033 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 419us/sample - loss: 0.9941 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 359us/sample - loss: 0.9837 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 299us/sample - loss: 0.9737 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 246us/sample - loss: 0.9634 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 246us/sample - loss: 0.9531 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 306us/sample - loss: 0.9434 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 359us/sample - loss: 0.9333 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.9242 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 286us/sample - loss: 0.9154 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 279us/sample - loss: 0.9072 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 253us/sample - loss: 0.8991 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 0.8921 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 313us/sample - loss: 0.8852 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 226us/sample - loss: 0.8790 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 426us/sample - loss: 0.8732 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 199us/sample - loss: 0.8679 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 366us/sample - loss: 0.8628 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.8580 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 618us/sample - loss: 0.8538 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 525us/sample - loss: 0.8496 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 459us/sample - loss: 0.8457 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 326us/sample - loss: 0.8422 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 206us/sample - loss: 0.8387 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.8355 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.8323 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.8293 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.8264 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.8237 - sparse_categorical_accuracy: 0.6667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.8209 - sparse_categorical_accuracy: 0.6733\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 199us/sample - loss: 0.8183 - sparse_categorical_accuracy: 0.6733\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.8158 - sparse_categorical_accuracy: 0.6800\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 253us/sample - loss: 0.8132 - sparse_categorical_accuracy: 0.6933\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 206us/sample - loss: 0.8108 - sparse_categorical_accuracy: 0.6933\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 0.8082 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.8057 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 273us/sample - loss: 0.8034 - sparse_categorical_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.8009 - sparse_categorical_accuracy: 0.7067\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 0.7988 - sparse_categorical_accuracy: 0.7333\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.7959 - sparse_categorical_accuracy: 0.7533\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 226us/sample - loss: 0.7934 - sparse_categorical_accuracy: 0.7533\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.7909 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7884 - sparse_categorical_accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7856 - sparse_categorical_accuracy: 0.8267\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.7831 - sparse_categorical_accuracy: 0.8467\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 226us/sample - loss: 0.7802 - sparse_categorical_accuracy: 0.8533\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.7776 - sparse_categorical_accuracy: 0.8600\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.7748 - sparse_categorical_accuracy: 0.8867\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 226us/sample - loss: 0.7718 - sparse_categorical_accuracy: 0.8933\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 206us/sample - loss: 0.7692 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7662 - sparse_categorical_accuracy: 0.9000\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 226us/sample - loss: 0.7631 - sparse_categorical_accuracy: 0.9067\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 219us/sample - loss: 0.7603 - sparse_categorical_accuracy: 0.9533\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 1ms/sample - loss: 0.7577 - sparse_categorical_accuracy: 0.9533\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 439us/sample - loss: 0.7544 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 359us/sample - loss: 0.7518 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.7488 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 219us/sample - loss: 0.7457 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.7431 - sparse_categorical_accuracy: 0.9600\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 219us/sample - loss: 0.7397 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 206us/sample - loss: 0.7369 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7337 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 0.7314 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.7286 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.7257 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.7228 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7199 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.7171 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7148 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7119 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.7094 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7073 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.7042 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 199us/sample - loss: 0.7017 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.6995 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 213us/sample - loss: 0.6979 - sparse_categorical_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.6948 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 199us/sample - loss: 0.6926 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.6903 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 352us/sample - loss: 0.6884 - sparse_categorical_accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 239us/sample - loss: 0.6861 - sparse_categorical_accuracy: 0.9800\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.6844 - sparse_categorical_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x202862bc5c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# keras\n",
    "model.fit(data, labels, batch_size=32, epochs=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "my_dense (MyDense)           (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh (TensorFlow [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "my_dense_1 (MyDense)         (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorF [(None, 3)]               0         \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model_tf_version.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_custom_objects = {\n",
    "    \"MyDense\" :  MyDense,\n",
    "}\n",
    "new_model = tf.keras.models.load_model(\"keras_model_tf_version.h5\",custom_objects=_custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 2, 2, 2,\n",
       "       2, 0, 1, 2, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2,\n",
       "       2, 0, 1, 1, 2, 2, 2, 2, 2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 1, 2, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 2, 2, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0,\n",
       "       2, 2, 1, 1, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2,\n",
       "       2, 1, 0, 1, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = new_model.predict(data)\n",
    "np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 0., 2., 2., 1., 1., 2., 0., 2., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 2., 2., 2., 0., 1., 2., 1., 1., 1., 2., 0., 1., 1., 2.,\n",
       "       2., 1., 1., 2., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       2., 0., 0., 1., 1., 0., 2., 2., 0., 2., 2., 2., 0., 2., 2., 2., 0.,\n",
       "       1., 1., 2., 2., 2., 2., 2., 0., 0., 1., 2., 1., 0., 2., 1., 0., 1.,\n",
       "       2., 1., 1., 0., 1., 0., 0., 0., 1., 1., 2., 2., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 2., 1., 0., 0., 1., 0., 2., 2., 1., 1., 2., 0., 1., 1., 2.,\n",
       "       2., 2., 0., 0., 2., 0., 0., 2., 0., 1., 2., 0., 2., 2., 1., 0., 1.,\n",
       "       2., 0., 0., 0., 1., 0., 2., 2., 0., 0., 2., 2., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
